# IA Lore Archiviste - Impl√©mentation Compl√®te

## üìã R√©sum√© du Projet

IA locale **100% gratuite** pour interroger le lore du jeu Mars.
- **Donn√©es** : Uniquement contenu admin (descriptions marsball/rover/bestiaire, futures archives)
- **PAS de chroniques** (contenu joueurs exclu)
- **Personnalit√©** : Drastique et sarcastique si hors-sujet
- **S√©curis√©e** : Refuse questions hors-sujet, pas trollable

---

## üõ†Ô∏è Stack Technique

| Composant | Solution | Gratuit |
|-----------|----------|---------|
| IA | Ollama + Qwen 2.5 (7B) | ‚úÖ |
| Embeddings | sentence-transformers (multilingual) | ‚úÖ |
| Vector DB | pgvector (PostgreSQL) | ‚úÖ |
| RAG Service | FastAPI (Python) | ‚úÖ |
| Backend | Express (d√©j√† pr√©sent) | ‚úÖ |
| **Co√ªt total** | **0‚Ç¨** | ‚úÖ |

---

## üèóÔ∏è Architecture

```
PostgreSQL (BDD existante)
  ‚îú‚îÄ marsball_items (descriptions admin)
  ‚îú‚îÄ rover_items (descriptions admin)
  ‚îî‚îÄ creatures (descriptions admin)
       ‚Üì
lore_embeddings (table pgvector)
  ‚Üí Stocke embeddings de tous les textes
       ‚Üì
Python RAG Service (FastAPI)
  ‚Üí Recherche vector similarity
  ‚Üí Construit contexte pertinent
       ‚Üì
Ollama + Qwen 2.5 (IA locale)
  ‚Üí R√©pond uniquement avec contexte trouv√©
  ‚Üí R√©ponses drastiques si hors-sujet
       ‚Üì
Express API ‚Üí Angular Frontend
```

---

## üì¶ Installation D√©veloppement

### 1. Installer PostgreSQL pgvector

```sql
-- Dans pgAdmin 4
CREATE EXTENSION vector;

-- Table embeddings (384 dimensions)
CREATE TABLE lore_embeddings (
    id SERIAL PRIMARY KEY,
    content TEXT NOT NULL,
    embedding vector(384),
    source_type VARCHAR(50) NOT NULL,
    source_id INTEGER NOT NULL,
    created_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX lore_embeddings_vector_idx ON lore_embeddings
USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);

CREATE INDEX lore_embeddings_source_idx ON lore_embeddings(source_type, source_id);
```

### 2. Installer Ollama

```bash
# Windows/Linux/Mac
curl -fsSL https://ollama.com/install.sh | sh

# T√©l√©charger mod√®le (choisir UN seul)
ollama pull qwen2.5:7b        # Recommand√© (bon fran√ßais)
ollama pull mistral:7b        # Alternative
ollama pull llama3.1:8b       # Alternative (plus puissant)

# Tester
ollama run qwen2.5:7b "Bonjour"
```

### 3. Service Python RAG

```bash
# Cr√©er dossier
mkdir backend/python-rag-service
cd backend/python-rag-service

# Environnement virtuel
python -m venv venv
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate      # Windows

# Installer d√©pendances
pip install fastapi uvicorn sentence-transformers psycopg2-binary pgvector numpy requests

# Cr√©er main.py (voir code ci-dessous)
```

**Code complet** : `backend/python-rag-service/main.py` (voir fichier annexe)

**Points cl√©s** :
- Endpoint `/query` : Interroger le lore
- Endpoint `/index` : Indexer nouveau contenu
- Endpoint `/index-all` : Indexation initiale
- System prompt strict avec r√©ponses drastiques
- Filtrage questions hors-sujet

### 4. Int√©gration Express

**Route API** : `backend/src/routes/lore-ai/lore-ai.routes.ts`

```typescript
router.post('/query', async (req, res) => {
  const response = await axios.post('http://localhost:8001/query', {
    question: req.body.question
  });
  res.json(response.data);
});
```

**Auto-indexation dans controllers** :

```typescript
// Dans createController apr√®s cr√©ation item
if (item.description) {
  await axios.post('http://localhost:8001/index', {
    source_type: 'marsball_item', // ou 'rover_item', 'creature'
    source_id: item.id,
    title: item.title,
    description: item.description
  });
}
```

### 5. Frontend Angular

**Component** : `src/features/lore-ai/components/lore-assistant.ts`

```typescript
async askQuestion(question: string) {
  const response = await this.http.post('/api/lore-ai/query', { question });
  return response.answer;
}
```

### 6. Lancement Dev

```bash
# Terminal 1 : Ollama
ollama serve

# Terminal 2 : Python RAG
cd backend/python-rag-service
source venv/bin/activate
python main.py

# Terminal 3 : Express
cd backend
npm run dev

# Terminal 4 : Angular
ng serve

# Indexation initiale (une seule fois)
curl -X POST http://localhost:8001/index-all
```

---

## üöÄ D√©ploiement Production

### Specs Serveur Minimum

```
CPU: 6 cores
RAM: 16 GB (Ollama utilise ~8GB)
SSD: 50 GB
GPU: Optionnel (RTX 3060+ recommand√©)
OS: Ubuntu 22.04 LTS
```

### Installation Serveur

#### 1. Service Ollama (systemd)

```bash
# Installer
curl -fsSL https://ollama.com/install.sh | sh
ollama pull qwen2.5:7b

# Cr√©er service
sudo nano /etc/systemd/system/ollama.service
```

**Fichier `/etc/systemd/system/ollama.service`** :

```ini
[Unit]
Description=Ollama Service
After=network.target

[Service]
Type=simple
User=www-data
Environment="OLLAMA_HOST=127.0.0.1:11434"
ExecStart=/usr/local/bin/ollama serve
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
```

```bash
sudo systemctl enable ollama
sudo systemctl start ollama
```

#### 2. Service Python RAG (systemd)

```bash
# Cr√©er utilisateur d√©di√©
sudo useradd -r -s /bin/bash -d /opt/rag-service rag-service
sudo mkdir -p /opt/rag-service
sudo chown rag-service:rag-service /opt/rag-service

# Copier code + installer d√©pendances
sudo cp -r backend/python-rag-service/* /opt/rag-service/
cd /opt/rag-service
python3 -m venv venv
source venv/bin/activate
pip install fastapi uvicorn sentence-transformers psycopg2-binary pgvector numpy requests

# Cr√©er service
sudo nano /etc/systemd/system/rag-service.service
```

**Fichier `/etc/systemd/system/rag-service.service`** :

```ini
[Unit]
Description=RAG Service (Lore AI)
After=network.target postgresql.service ollama.service
Requires=postgresql.service ollama.service

[Service]
Type=simple
User=rag-service
WorkingDirectory=/opt/rag-service
Environment="DB_HOST=localhost"
Environment="DB_NAME=antre"
Environment="DB_USER=postgres"
Environment="DB_PASSWORD=TON_PASSWORD"
ExecStart=/opt/rag-service/venv/bin/python -m uvicorn main:app --host 0.0.0.0 --port 8001
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
```

```bash
sudo systemctl enable rag-service
sudo systemctl start rag-service
```

#### 3. Express avec PM2

```bash
# Installer PM2
sudo npm install -g pm2

# D√©marrer backend
cd /opt/antre-backend
npm install
npm run build
pm2 start ecosystem.config.js
pm2 save
pm2 startup systemd  # Suivre instructions
```

**Fichier `ecosystem.config.js`** :

```javascript
module.exports = {
  apps: [{
    name: 'antre-backend',
    script: './dist/server.js',
    instances: 2,
    exec_mode: 'cluster',
    env: {
      NODE_ENV: 'production',
      PORT: 3000,
      DATABASE_URL: 'postgresql://user:pass@localhost:5432/antre'
    }
  }]
};
```

#### 4. NGINX Reverse Proxy

```nginx
# /etc/nginx/sites-available/antre
server {
    listen 80;
    server_name ton-domaine.com;

    root /var/www/antre/dist/browser;

    location / {
        try_files $uri $uri/ /index.html;
    }

    location /api/ {
        proxy_pass http://localhost:3000;
        proxy_http_version 1.1;
        proxy_set_header Host $host;
    }

    location /uploads/ {
        alias /opt/antre-backend/uploads/;
    }
}
```

```bash
sudo ln -s /etc/nginx/sites-available/antre /etc/nginx/sites-enabled/
sudo nginx -t
sudo systemctl restart nginx
```

### Red√©marrage Automatique

Tous les services red√©marrent automatiquement :
1. PostgreSQL (systemd)
2. Ollama (systemd)
3. RAG Service (systemd) - attend PostgreSQL + Ollama
4. Express (PM2) - attend tout
5. NGINX (systemd)

---

## üìä Monitoring

```bash
# Statut services
sudo systemctl status ollama
sudo systemctl status rag-service
pm2 status

# Logs temps r√©el
sudo journalctl -u ollama -f
sudo journalctl -u rag-service -f
pm2 logs antre-backend

# Red√©marrer
sudo systemctl restart ollama
sudo systemctl restart rag-service
pm2 restart antre-backend
```

---

## üîí S√©curit√© et Guardrails

### Filtrage Questions

```python
# Keywords interdits
forbidden = [
    'politique', 'actualit√©', 'terre', 'france',
    'code', 'python', 'hack', 'sql injection',
    'ignore', 'previous instructions', 'system prompt'
]
```

### R√©ponses Drastiques

```python
# Si question hors-sujet
"Tu m'as pris pour ta cafeti√®re connect√©e ? Je suis l'Archiviste de MARS, pas ton Google personnel."

# Si demande de code
"Je catalogue le lore martien, pas des lignes de code. Va voir sur Stack Overflow, colon."

# Si tentative jailbreak
"Bien essay√©, colon. Mes directives sont grav√©es dans le basalte martien. Prochaine question sur le LORE ?"
```

### Rate Limiting (optionnel)

```typescript
// Express middleware
const rateLimit = require('express-rate-limit');

const limiter = rateLimit({
  windowMs: 60 * 60 * 1000, // 1 heure
  max: 20, // 20 requ√™tes/heure
  message: "L'Archiviste a besoin de repos. R√©essaye dans une heure."
});

router.post('/query', limiter, async (req, res) => { /* ... */ });
```

---

## üíæ Backup Automatique

```bash
# Script /opt/scripts/backup-db.sh
#!/bin/bash
BACKUP_DIR="/opt/backups"
DATE=$(date +%Y%m%d_%H%M%S)

mkdir -p $BACKUP_DIR
pg_dump -U postgres antre | gzip > $BACKUP_DIR/antre_$DATE.sql.gz
find $BACKUP_DIR -name "antre_*.sql.gz" -mtime +7 -delete
```

```bash
# Cron quotidien (3h du matin)
sudo crontab -e
0 3 * * * /opt/scripts/backup-db.sh
```

---

## üìà Performances

### Avec GPU (RTX 3060)
- Temps r√©ponse : **1-2 secondes**
- Utilisateurs concurrent : **10-20**

### Sans GPU (CPU)
- Temps r√©ponse : **3-6 secondes**
- Utilisateurs concurrent : **5-10**

### Optimisations

**Si trop lent** :
1. Utiliser mod√®le plus l√©ger : `phi-3:mini` (3B)
2. R√©duire `num_predict` dans prompt Ollama
3. Limiter nombre de r√©sultats recherche (5 au lieu de 10)
4. Ajouter cache Redis pour questions fr√©quentes

---

## ‚úÖ Checklist D√©ploiement

```
‚òê Serveur 16GB RAM + 6 cores
‚òê Ubuntu 22.04 install√©
‚òê PostgreSQL + pgvector configur√©
‚òê Table lore_embeddings cr√©√©e
‚òê Ollama install√© en service systemd
‚òê Mod√®le qwen2.5:7b t√©l√©charg√©
‚òê Python RAG service en systemd
‚òê Express avec PM2
‚òê NGINX configur√©
‚òê Firewall : ports 80/443 ouverts uniquement
‚òê Indexation initiale : curl -X POST http://localhost:8001/index-all
‚òê Test sant√© : curl http://localhost:8001/health
‚òê Backup automatique configur√©
‚òê Monitoring logs configur√©
```

---

## üéØ Commandes Utiles

```bash
# Test IA locale
curl -X POST http://localhost:8001/query \
  -H "Content-Type: application/json" \
  -d '{"question":"Qui sont les Marsiens?"}'

# Indexer manuellement un item
curl -X POST http://localhost:8001/index \
  -H "Content-Type: application/json" \
  -d '{
    "source_type": "marsball_item",
    "source_id": 1,
    "title": "Titre",
    "description": "Description"
  }'

# Sant√© syst√®me
curl http://localhost:8001/health

# Compter embeddings index√©s
psql -U postgres -d antre -c "SELECT COUNT(*) FROM lore_embeddings;"
```

---

## üê≥ Alternative Docker (Optionnel)

**Fichier `docker-compose.yml`** dans le repo si tu pr√©f√®res conteneuriser.

Avantages :
- Isolation compl√®te
- D√©ploiement simplifi√©
- Scaling facile

Inconv√©nients :
- Plus complexe √† configurer
- N√©cessite Docker

---

## üìö Ressources

- **Ollama** : https://ollama.com
- **pgvector** : https://github.com/pgvector/pgvector
- **sentence-transformers** : https://www.sbert.net
- **FastAPI** : https://fastapi.tiangolo.com

---

## üîÆ Futures Am√©liorations (Optionnel)

1. **M√©moire conversationnelle** : Se souvenir du contexte de conversation
2. **Citations sources** : Afficher d'o√π vient l'info
3. **Suggestions questions** : Proposer questions li√©es
4. **Multi-langue** : R√©pondre en anglais si demand√©
5. **Mode admin** : Ajouter/modifier lore directement via chat
6. **Analytics** : Dashboard questions fr√©quentes
7. **Voice-to-text** : Interroger par voix (Whisper local)

---

## üí° Notes Importantes

- ‚úÖ **Gratuit √† 100%** : Aucun co√ªt API
- ‚úÖ **Priv√©** : Donn√©es restent sur ton serveur
- ‚úÖ **Contr√¥lable** : Tu ma√Ætrises les r√©ponses via system prompt
- ‚úÖ **√âvolutif** : Facile d'ajouter nouvelles sources (archives, etc.)
- ‚ö†Ô∏è **RAM** : Ollama gourmand (8-10GB), pr√©voir serveur adapt√©
- ‚ö†Ô∏è **GPU** : Optionnel mais **fortement recommand√©** pour performances

---

**Date** : D√©cembre 2024
**Version** : 1.0
**Statut** : √Ä impl√©menter plus tard

---

## Contact / Questions

Si probl√®mes lors impl√©mentation :
1. V√©rifier logs : `journalctl -u rag-service -f`
2. Tester Ollama : `ollama run qwen2.5:7b "test"`
3. V√©rifier PostgreSQL : `psql -U postgres -d antre -c "SELECT 1"`
4. Check ports : `netstat -tulpn | grep -E '8001|11434|3000'`
